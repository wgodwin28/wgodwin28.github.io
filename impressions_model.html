<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Predicting Impressions</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">HOME</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="projects.html">Data Projects</a>
</li>
<li>
  <a href="papers.html">Papers</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="http://github.com/wgodwin28">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/will-godwin-03a106150/">
    <span class="fa fa-linkedin fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Predicting Impressions</h1>
<h4 class="date">9/2/2019</h4>

</div>


<p>##Start-up</p>
<p>The goal is to build a model that predicts “impressions”, or the number of people the ad reached, using relevant advertisement covariates. I’ll use political advertisement spending data released by google. The dataset contains over 200,000 unique ads since June 2018 with relevant metadata including <em>number of impressions</em> as a ordinal categorical variable. Intially, I need to figure out useful predictors in the model by investigating the univariate distributions and basic bivariate relationships.</p>
<pre class="r"><code>#find date when data was downloaded last
dwnld_date &lt;- file.info(&quot;~/Desktop/google_ads/data/google-political-ads-transparency-bundle/&quot;)$atime %&gt;% as.Date()

#if not downloaded in past 7 days, re-download
if(dwnld_date &lt; Sys.Date() - 7){
  #set url and file destination
  url &lt;- &quot;https://storage.googleapis.com/transparencyreport/google-political-ads-transparency-bundle.zip&quot;
  dest &lt;- &quot;~/Desktop/google_ads/data/google_ads.zip&quot;
  unzip_dir &lt;- &quot;~/Desktop/google_ads/data&quot;
  
  #download ads data and save to central location
  download.file(url=url, destfile = dest, method = &quot;curl&quot;)
  unzip(dest, exdir = unzip_dir)
}

#read in data
dt &lt;- read_csv(&quot;~/Desktop/google_ads/data/google-political-ads-transparency-bundle/google-political-ads-creative-stats.csv&quot;)

#relevant variables
vars_keep &lt;- c(&quot;Ad_Type&quot;, &quot;Regions&quot;, &quot;Advertiser_ID&quot;, &quot;Date_Range_Start&quot;, &quot;Date_Range_End&quot;, 
               &quot;Num_of_Days&quot;, &quot;Impressions&quot;, &quot;Spend_Range_Min_USD&quot;, &quot;Spend_Range_Max_USD&quot;)

#subset to variables of interest and create useful variables
dt &lt;- dt %&gt;%
  dplyr::select(vars_keep) %&gt;% #keep relevant variables
  filter(!is.na(Num_of_Days) &amp; !is.na(Regions) &amp; !is.na(Ad_Type)) %&gt;%
  mutate(startYear = substr(Date_Range_Start,1,4), #variable indicating year ad started
         Region = ifelse(grepl(&quot;EU&quot;, Regions), &quot;EU&quot;, &quot;US&quot;) %&gt;% as.factor(), #create cleaner regions variable
         month_year = format(as.Date(Date_Range_Start), &quot;%m-%Y&quot;),
         week_start = floor_date(as.Date(Date_Range_Start), unit = &quot;week&quot;),
         week_end = floor_date(as.Date(Date_Range_End), unit = &quot;week&quot;),
         cost_cat = paste0(Spend_Range_Min_USD, &quot;-&quot;, Spend_Range_Max_USD) %&gt;% 
           factor(levels=c(&quot;0-100&quot;, &quot;100-1000&quot;, &quot;1000-50000&quot;, &quot;50000-100000&quot;, &quot;100000-NA&quot;),
                  labels = c(&quot;0-100&quot;, &quot;100-1k&quot;, &quot;1k-50k&quot;, &quot;50k-100k&quot;, &quot;100k+&quot;)),
         Spend_Range_Min_USD = as.factor(Spend_Range_Min_USD),
         Ad_Type = as.factor(Ad_Type),
         Impressions = factor(Impressions, 
                              levels = c(&quot;≤ 10k&quot;, &quot;10k-100k&quot;, &quot;100k-1M&quot;, &quot;1M-10M&quot;, &quot;&gt; 10M&quot;),
                              labels = c(&quot;Under 10k&quot;, &quot;10k-100k&quot;, 
                                         &quot;100k-1M&quot;, &quot;1M-10M&quot;, &quot;10M+&quot;)))

  #filter(regions==&quot;US&quot;) #only include US data
dt &lt;- dt %&gt;%
  mutate(
    case_when(
      cost_cat==&quot;100000-NA&quot; ~ &quot;100k+&quot;
    )
  )</code></pre>
<p>Load in the dataset and prep it for visualizing and modeling.</p>
<p>##Data Exploration ###Impressions</p>
<pre class="r"><code>#impressions
ggplot(dt, aes(Impressions)) +
  geom_bar() +
  geom_text(stat = &#39;count&#39;, aes(label = percent(..count../nrow(dt)), vjust = -0.2)) +
  theme_bw()</code></pre>
<p><img src="impressions_model_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Most ads are classified under the “Under 10k” category, indicating an imbalanced classification problem. Due to this imbalance, I’ll plot impression counts on the log scale to observe differences at smaller frequecies.</p>
<p>###Ad cost One might presume that money spent for an ad would be an excellent predictor of impressions, given that that most tech platforms follow an advertising model of more money spent = more exposure. Google provides dollars spent as a categorical variable of 5 bins. Let’s see how well ad dollars spent and number of impressions correlate.</p>
<pre class="r"><code>table(dt$Impressions, dt$cost_cat)</code></pre>
<pre><code>##            
##              0-100 100-1k 1k-50k 50k-100k  100k+
##   Under 10k 206644  10295    696        0      0
##   10k-100k   22267  15681   4734        0      0
##   100k-1M     2934   6878   8018        0      0
##   1M-10M        22    656   2247        0      0
##   10M+           0      0    284        0      0</code></pre>
<p>Since “ad cost” and “impressions” variables are both ordinal, 5-bin variables, if one was to map the categories to each other on 1 to 1 basis, we could calculate accuracy of a model that solely used “ad cost” to predict “impressions”. And turns out, it would be correct 61.4% of the time! This tells us two things: money spent on ads is a primary driver of total impressions <em>but</em> it’s not the only driver. Maybe we can find other predictors in the data set that could improve our future model’s accuracy.</p>
<p>###Temporal trends</p>
<pre class="r"><code>#time series plot of ad counts across impression category
dt %&gt;%
  group_by(date=as.Date(week_start), Impressions) %&gt;%
  summarize(weekly_ad_count=n()) %&gt;%
  ggplot(aes(date, weekly_ad_count, color=Impressions)) +
    geom_point() +
    geom_smooth(method = &quot;loess&quot;) +
    xlab(&quot;Date&quot;) +
    ylab(&quot;Number of Ads (on log scale)&quot;) +
    geom_vline(xintercept = as.Date(&quot;2018/11/06&quot;), linetype=4) +
    geom_text(aes(x=as.Date(&quot;2018/10/28&quot;), y=9400, label=&quot; Midterm&quot;), 
              colour=&quot;blue&quot;, angle=90, text=element_text(size=11)) +
    scale_y_log10() +
    scale_x_date(labels = date_format(&quot;%m/%Y&quot;), breaks = date_breaks(&quot;2 month&quot;)) +
    scale_color_discrete(name = &quot;Impressions&quot;) +
    theme_bw()</code></pre>
<p><img src="impressions_model_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>In order to assess whether date of ad may be related to number of impressions, I aggregated the number of ads by week of ad start date and impression category. Then I plotted the number of ads across the full time series, colored by impression category, and fit a loess smoother through each. The loess, in this case, is a helpful way to explore time trends in the data.</p>
<p>There does seem to be some relationship, albeit non-linear, between number of ads and ad start date. Ad counts, regardless of impression category, appear to increase up the the 2018 midterm election. However, the time trend is similar across the different impression categories-note the similar shapes for each loess curve. In other words, the relative fraction each impression category comprises, with reference to the impression envelope, does not change much over time. So I’ll hold off on including a time variable in my model for now.</p>
<p>###Impressions by region</p>
<pre class="r"><code>#plot impression counts by region
ggplot(dt, aes(Impressions, fill=Region)) +
  geom_bar(position = &quot;dodge&quot;) +
  scale_y_log10() +
  ylab(&quot;Count (log 10 scale)&quot;) +
  theme_bw()</code></pre>
<p><img src="impressions_model_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Above is a bar chart showing counts of each impression category across the two regions: EU and US. Note that these frequencies were plotted on a log 10 scale in order to illustrate relative frequencies within the lower frequency categories (100k-1M impressions, etc). The EU has fewer ads within each category and appears to have a greater proportion of its ads with &gt;10k impressions compared to the U.S., which indicates that this variable may be a useful covariate in the model.</p>
<p>###Impressions by ad type</p>
<pre class="r"><code>#plot impression counts by type of ad
ggplot(dt, aes(Impressions, fill=Ad_Type)) +
  geom_bar(position = &quot;dodge&quot;) +
  scale_y_log10() +
  ylab(&quot;Count (log 10 scale)&quot;) +
  theme_bw()</code></pre>
<p><img src="impressions_model_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>This figure shows counts (on log 10 scale) of ads across impression category for each type of ad. “Image” ads remain the most popular type across impression category, while “Text” ads significantly decrease as number of impressions increase, relatively to other ad types.</p>
<p>###Days ad aired</p>
<pre class="r"><code>dt %&gt;%
  mutate(days_running=difftime(Date_Range_End, Date_Range_Start, units = &quot;days&quot;)) %&gt;%
  ggplot(aes(Impressions, days_running)) +
    geom_boxplot() +
    ylab(&quot;Number of Days Aired&quot;) +
    theme_bw()</code></pre>
<p><img src="impressions_model_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>This figure shows box and whiskers of number of days ad was aired across impression category. While these data do look noisy, we see indication of a potential trend: as ad air time increases so does the number of impressions.</p>
<p>Based on the exploratory plots and tables, the covariates we’ll use to predict impressions are: <em>cost of the ad</em>, <em>ad type</em> (text, video, or image), <em>region the ad aired</em> (U.S. or E.U.), and <em>number of days the ad aired</em>.</p>
<p>##Modeling</p>
<pre class="r"><code>#model reponse and covariates
response &lt;- &quot;Impressions&quot;
covariates &lt;- c(&quot;Num_of_Days&quot;, &quot;Region&quot;, &quot;Ad_Type&quot;, &quot;cost_cat&quot;)

#create training data frame
set.seed(2^9)
train &lt;- sample(c(TRUE, FALSE), nrow(dt), rep=TRUE)
dt.train &lt;- dt %&gt;%
  dplyr::select(response, covariates) %&gt;%
  filter(train &amp; !is.na(cost_cat))

dt.test &lt;- dt %&gt;%
  dplyr::select(response, covariates) %&gt;%
  filter(!train &amp; !is.na(cost_cat))

#extract the testing response and predictors for prediction
x.test &lt;- dt.test %&gt;% dplyr::select(covariates)
y &lt;- dt.test %&gt;% dplyr::select(response)

#create standard formula object
mod.formula &lt;- as.formula(paste(response, 
                            paste(covariates, collapse = &quot;+&quot;), sep = &quot; ~ &quot;))</code></pre>
<p>We’ll model impressions using 3 different methods: logistic regression and two tree-based methods. Logistic is helpful as a first pass since it generally performs well and produces interpretable coefficients. The data will be ramdomly split into a training and testing set for evaluation of model performance.</p>
<p>###Multinomial logistic regression</p>
<pre class="r"><code>#Build the model
model1 &lt;- vglm(formula = mod.formula, 
             data = dt.train, family = &quot;multinomial&quot;)

#extract model summary
#summary(model1)

#Predict using the model
probability &lt;- predict(model1, x.test, type=&quot;response&quot;)
dt.test &lt;- dt.test %&gt;%
  mutate(predicted_cat = apply(probability, 1, which.max),
         predicted_name = case_when(predicted_cat==1 ~ &quot;Under 10k&quot;,
                                    predicted_cat==2 ~ &quot;10k-100k&quot;,
                                    predicted_cat==3 ~ &quot;100k-1M&quot;,
                                    predicted_cat==4 ~ &quot;1M-10M&quot;,
                                    predicted_cat==5 ~ &quot;10M+&quot;),
         predicted_name = factor(predicted_name, 
                                 levels = c(&quot;Under 10k&quot;, &quot;10k-100k&quot;, 
                                            &quot;100k-1M&quot;, &quot;1M-10M&quot;, &quot;10M+&quot;)))

#Accuracy of the model
mtab &lt;- table(dt.test$predicted_name, dt.test$Impressions)
confusionMatrix(mtab)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            
##             Under 10k 10k-100k 100k-1M 1M-10M   10M+
##   Under 10k    107009    11450    1420     20      0
##   10k-100k       1613     8482    2630    163      1
##   100k-1M           8     1435    4761   1073    121
##   1M-10M            0        0      62    160     17
##   10M+              0        0       0      0      0
## 
## Overall Statistics
##                                           
##                Accuracy : 0.8575          
##                  95% CI : (0.8556, 0.8593)
##     No Information Rate : 0.7736          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5577          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: Under 10k Class: 10k-100k Class: 100k-1M
## Sensitivity                    0.9851         0.39697        0.53657
## Specificity                    0.5946         0.96298        0.97995
## Pos Pred Value                 0.8925         0.65808        0.64355
## Neg Pred Value                 0.9210         0.89897        0.96909
## Prevalence                     0.7736         0.15216        0.06319
## Detection Rate                 0.7620         0.06040        0.03390
## Detection Prevalence           0.8538         0.09179        0.05268
## Balanced Accuracy              0.7898         0.67998        0.75826
##                      Class: 1M-10M Class: 10M+
## Sensitivity               0.112994   0.0000000
## Specificity               0.999432   1.0000000
## Pos Pred Value            0.669456         NaN
## Neg Pred Value            0.991040   0.9990101
## Prevalence                0.010084   0.0009899
## Detection Rate            0.001139   0.0000000
## Detection Prevalence      0.001702   0.0000000
## Balanced Accuracy         0.556213   0.5000000</code></pre>
<p>The confusion matrix shows the model predictions (row-wise) stacked against the actual data (column-wise). If the model fit the data perfectly, we’d only see values along the diagonal and would see zeros everywhere else. The overall accuracy is 85%, indicating that the model correctly labels the testing data 85% of the time. Sensitivity (we’ll use “recall”) and specificity varies across the impression categories, as we’d expect. When predicting “Under 10k” impressions, a recall of 0.98 indicates that the model get 98% of the actual “Under 10k” impressions correct. The relatively poor specificity indicates that the model correctly predicts that an ad will NOT get “Under 10k” impressions 63% of the time. We see the opposite result from the “10M+” impressions category. Because we have an imbalanced classification problem with so few “10M+” impression ads, the model can predict that an ad will not get “10M+” impressions with 99.99% confidence. However, recall of 0.13 reveals that 87% of actual “10M+” impression ads are incorrectly labeled by the model.</p>
<p>Model evaluation can be based on overall accuracy of the model or on more specific metrics like precision or recall, depending on the research question. For instance, consider that the goal of many political action committees (PAC) is to reach as many people as possible using as little money as possible. In order to figure out how to do this, they could start with looking at what covariates are conditionally associated with number of impressions from the logistic model coefficients. We can see that even controlling for ad cost, an ad with longer air time tends to achieve more impression. Going even further, they could build a model that optimizes for their main goal-accurately predicting ads with millions of impressions or recall. As we noted, the logistic model above has poor recall for impression categories of particular interest as it correctly labels a “1-10M” impression ad only 10% of the time and a “10M+” impression ad only 13% of the time. Compare this to our original “model, that simply used the cost of ad to predict impressions, which had a recall for”10M+" of 9%. Both models leave room for improvement. Let’s see if we can improve the recall at the high impression end using a tree-based models.</p>
<p>###Random Forest</p>
<pre class="r"><code>#Build the model
model2 &lt;- randomForest(mod.formula, data = dt.train)

#Summarize the model
#summary(model2)

#Predict using the model
dt.test$pred_randomforest &lt;- predict(model2, x.test)

#Accuracy of the model
mtab2 &lt;- table(dt.test$pred_randomforest, dt.test$Impressions)
confusionMatrix(mtab2)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            
##             Under 10k 10k-100k 100k-1M 1M-10M   10M+
##   Under 10k    107403    11749    1460     20      0
##   10k-100k       1217     8048    2320    152      1
##   100k-1M          10     1569    4989    982     74
##   1M-10M            0        1     100    260     39
##   10M+              0        0       4      2     25
## 
## Overall Statistics
##                                           
##                Accuracy : 0.8597          
##                  95% CI : (0.8579, 0.8615)
##     No Information Rate : 0.7736          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5606          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: Under 10k Class: 10k-100k Class: 100k-1M
## Sensitivity                    0.9887         0.37666        0.56227
## Specificity                    0.5839         0.96901        0.97997
## Pos Pred Value                 0.8903         0.68564        0.65438
## Neg Pred Value                 0.9380         0.89650        0.97075
## Prevalence                     0.7736         0.15216        0.06319
## Detection Rate                 0.7648         0.05731        0.03553
## Detection Prevalence           0.8590         0.08359        0.05429
## Balanced Accuracy              0.7863         0.67283        0.77112
##                      Class: 1M-10M Class: 10M+
## Sensitivity               0.183616   0.1798561
## Specificity               0.998993   0.9999572
## Pos Pred Value            0.650000   0.8064516
## Neg Pred Value            0.991744   0.9991880
## Prevalence                0.010084   0.0009899
## Detection Rate            0.001852   0.0001780
## Detection Prevalence      0.002848   0.0002208
## Balanced Accuracy         0.591304   0.5899067</code></pre>
<p>Compared to multinomial logistic regression, random forest does perform better for lower frequency classes like “1-10M” and “10M+”. Despite doubling the recall observed in logistic regression, a model that correctly labels an ad to get “10M+” impressions only 30% of the time leaves a lot to be desired. Outside of the scope of this analysis but in an effort to improve recall, we could oversample the minority impression categories. This has the effect of balancing out the distribution of impressions and would likely improve prediction in minority categories. The overall accuracy of the random forest model is similar to the logisitic regression at 85%, indicating that the gains made in recall for high impression classes may have come at cost for other components of model performance.</p>
<p>###Boosted C5.0</p>
<pre class="r"><code>#Build the model
model3 &lt;- C5.0(mod.formula, data = dt.train, trials = 8)

#Predict using the model
dt.test$pred_c50 &lt;- predict(model3, x.test)

#Accuracy of the model
mtab3 &lt;- table(dt.test$pred_c50, dt.test$Impressions)
confusionMatrix(mtab3)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            
##             Under 10k 10k-100k 100k-1M 1M-10M   10M+
##   Under 10k    107180    11458    1421     20      0
##   10k-100k       1291     6026     724     18      1
##   100k-1M         159     3883    6728   1378    138
##   1M-10M            0        0       0      0      0
##   10M+              0        0       0      0      0
## 
## Overall Statistics
##                                           
##                Accuracy : 0.8541          
##                  95% CI : (0.8522, 0.8559)
##     No Information Rate : 0.7736          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.55            
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: Under 10k Class: 10k-100k Class: 100k-1M
## Sensitivity                    0.9867         0.28202        0.75826
## Specificity                    0.5943         0.98292        0.95775
## Pos Pred Value                 0.8926         0.74764        0.54762
## Neg Pred Value                 0.9287         0.88410        0.98326
## Prevalence                     0.7736         0.15216        0.06319
## Detection Rate                 0.7633         0.04291        0.04791
## Detection Prevalence           0.8551         0.05740        0.08749
## Balanced Accuracy              0.7905         0.63247        0.85800
##                      Class: 1M-10M Class: 10M+
## Sensitivity                0.00000   0.0000000
## Specificity                1.00000   1.0000000
## Pos Pred Value                 NaN         NaN
## Neg Pred Value             0.98992   0.9990101
## Prevalence                 0.01008   0.0009899
## Detection Rate             0.00000   0.0000000
## Detection Prevalence       0.00000   0.0000000
## Balanced Accuracy          0.50000   0.5000000</code></pre>
<p>A Boosted C5.0 model is based on simple tree-based framework that uses “boosting” methods. While a random forest splits the predictor space on into partitions that minimize impurity/maximize information criterion for each independent tree, boosting models grow trees sequentially with the residuals of the previous tree becoming the response variable of the subsequent tree. While this smoothing over residuals may sometimes improve model performance, in this context, the random forest performed slightly better overall.</p>
<p>There are a variety of other models one could use to classify impressions from naive Bayes to support vector machines, which could lead to improved overall accuracy and improved recall. There’s also feature engineering that we didn’t investigate at length (like lagged variables or midterm-related associations). Those pursuits are fodder for future projects. The takeaway from this analysis is that logistic regression, while some times not as accurate, still can construct a useful springboard for further analyses due to its interpretability. And regression trees-random forests and boosting methods-can be fast, flexible frameworks for optimizing toward a specific performance metric.</p>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
